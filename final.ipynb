{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260},{"sourceId":8246856,"sourceType":"datasetVersion","datasetId":4892782}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unstructured\n!pip install unstructured[pdf]\n!pip install -U langchain-openai\n!pip install chromadb\n!pip install pypdf\n!pip install tqdm\n!pip install sentence-transformers\n!pip install transformers\n!pip install InstructorEmbedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pinecone-client\n!pip install langchain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets spacy scikit-learn\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install pyngrok\n! pip install flask-ngrok\n! ngrok authtoken 2fgafRv4DMXaDwep9sSlHpsPgeB_76vC81WKH5ns7owcmDeWC\n! pip install streamlit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install cmake\n!pip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:20:20.259144Z","iopub.execute_input":"2024-05-06T15:20:20.260158Z","iopub.status.idle":"2024-05-06T15:20:57.183044Z","shell.execute_reply.started":"2024-05-06T15:20:20.260122Z","shell.execute_reply":"2024-05-06T15:20:57.181746Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"Collecting cmake\n  Downloading cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\nDownloading cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cmake\nSuccessfully installed cmake-3.29.2\nLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\nCollecting llama-cpp-python\n  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.69-cu121/llama_cpp_python-0.2.69-cp310-cp310-linux_x86_64.whl (141.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.8/141.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m784.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.69\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile fastllm.py\nimport os\nimport urllib.request\nfrom llama_cpp import Llama\n\ndef download_file(file_link, filename):\n    # Checks if the file already exists before downloading\n    if not os.path.isfile(filename):\n        urllib.request.urlretrieve(file_link, filename)\n        print(\"File downloaded successfully.\")\n    else:\n        print(\"File already exists.\")\n\n\n# Dowloading GGML model from HuggingFace\nggml_model_path = \"https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_0.gguf\"\nfilename = \"zephyr-7b-beta.Q4_0.gguf\"\n\ndownload_file(ggml_model_path, filename)\n\nfrom llama_cpp import Llama\n\nmodel_path='zephyr-7b-beta.Q4_0.gguf'\nllm = Llama(model_path=model_path,\n            n_gpu_layers=50,n_ctx=10000)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T17:06:27.721461Z","iopub.execute_input":"2024-05-06T17:06:27.722232Z","iopub.status.idle":"2024-05-06T17:06:27.729505Z","shell.execute_reply.started":"2024-05-06T17:06:27.722188Z","shell.execute_reply":"2024-05-06T17:06:27.728533Z"},"trusted":true},"execution_count":198,"outputs":[{"name":"stdout","text":"Overwriting fastllm.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.makedirs('/kaggle/working/pages')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T15:52:34.350408Z","iopub.execute_input":"2024-05-06T15:52:34.351116Z","iopub.status.idle":"2024-05-06T15:52:34.355636Z","shell.execute_reply.started":"2024-05-06T15:52:34.351066Z","shell.execute_reply":"2024-05-06T15:52:34.354742Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"os.remove('/kaggle/working/our_llm.py')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T17:55:02.505666Z","iopub.execute_input":"2024-05-06T17:55:02.506587Z","iopub.status.idle":"2024-05-06T17:55:02.512040Z","shell.execute_reply.started":"2024-05-06T17:55:02.506555Z","shell.execute_reply":"2024-05-06T17:55:02.511099Z"},"trusted":true},"execution_count":232,"outputs":[]},{"cell_type":"code","source":"!pip install streamlit-extras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile Homepage.py\nimport streamlit as st\n\nst.set_page_config(\n    page_title=\"Resume Improver\",\n    page_icon=\"ðŸ“œ\",\n)\n\nst.write(\"Welcome to LLM powered resume Improver\")\n\nst.sidebar.success(\"Select what you would like to do\")\n\nst.markdown(\n    \"\"\"\n    This resume improver is based on mistral-7b LLM. It has 2 features, namely Multiple Resume Comparison\n    and Resume And Job Description. Multiple Resume Comparison gives information about how the submitted resumes \n    differe from one another and which one is better. Resume And Job Description on the other hand requires one job \n    description and one resume which is to be improved on the basis of that Job Description. \n\"\"\"\n)\nif 'documents' not in st.session_state:\n    st.session_state['documents'] = None\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:12:58.590214Z","iopub.execute_input":"2024-05-06T18:12:58.591258Z","iopub.status.idle":"2024-05-06T18:12:58.598921Z","shell.execute_reply.started":"2024-05-06T18:12:58.591217Z","shell.execute_reply":"2024-05-06T18:12:58.597836Z"},"trusted":true},"execution_count":241,"outputs":[{"name":"stdout","text":"Overwriting Homepage.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile pages/1_Multiple_Resume_Comparison.py\nimport streamlit as st\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\nfrom fastllm import llm\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\npersist_directory = 'db'\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n\nst.set_page_config(\n    page_title=\"Multi Resume\",\n    page_icon=\"ðŸ“œ\"\n\n)\n\n# Sidebar for uploading PDF files\ndocs=[]\nwith st.sidebar:\n    st.title(\"Resume Comparison:\")\n    pdf_docs = st.file_uploader(\"Upload your resumes in the form of PDFs \", accept_multiple_files=True)\n    if pdf_docs is not None:\n        with st.spinner(\"Processing...\"):\n            for i,file in enumerate(pdf_docs):\n                name=\"resume_file\"+str(i)+\".pdf\"\n                with open(name, 'wb') as f:\n                    f.write(file.getvalue())\n                path=\"/kaggle/working/\"+name\n                loader = UnstructuredPDFLoader(path)\n                data = loader.load()\n                docs.append(data[0]) \n                #st.session_state['documents'] = docs\n            st.success(\"Done\")\n            \n#docs = st.session_state['documents']\nif len(docs)>0:\n    vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_directory)\n    vectordb.persist()\n    vectordb = None\n    vectordb = Chroma(persist_directory=persist_directory,embedding_function=embeddings)                \n    retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n    query = st.text_input(\"What are you looking for in these candidates?\")\n    if not query:\n        st.stop()\n    docs = retriever.invoke(query)\n\n    def split_docs(documents, chunk_size=1000, chunk_overlap=100):\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        docs = text_splitter.split_documents(documents)\n        return docs\n\n    docs=split_docs(docs)\n\n    prompt=\"Following is the conversation between the User and AI. AI is a hiring assistant chatbot.AI give short and very precise answers. AI's task is to answer the question based on the following resumes :\"\n\n    for i in range(5):\n        prompt=prompt+\"\\n\"+f\"{i+1}th Resume : \"+\"\\n\"+docs[i].page_content\n\n    history=prompt\n    input_text=\"\"\n    n=0\n    while query!=\"Quit\" or query!=\"quit\":\n        if n==0:\n            input_text=f\"Please explain why the said above resumes are more relevant with query : {query} \"\n        else:\n            input_text = st.text_input(\"What more would you like to ask?\",key=n)\n            if not input_text:\n                st.stop()\n\n        if input_text==\"quit\":\n            break\n\n        history=history+ f\"User : {input_text}\" + \"\\n\" + \"AI :\"\n        response=llm(history, max_tokens=300)\n\n        out=response[\"choices\"][0][\"text\"].strip()\n        lines = history.split('\\n')\n\n\n        new_last_line = f\"AI : {out}\"\n        n=n+1\n        lines[-1] = new_last_line\n        history='\\n'.join(lines)\n        i=history.rfind(\"AI:\")\n        st.write(out)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:13:11.292384Z","iopub.execute_input":"2024-05-06T18:13:11.292752Z","iopub.status.idle":"2024-05-06T18:13:11.300879Z","shell.execute_reply.started":"2024-05-06T18:13:11.292727Z","shell.execute_reply":"2024-05-06T18:13:11.299960Z"},"trusted":true},"execution_count":242,"outputs":[{"name":"stdout","text":"Overwriting pages/1_Multiple_Resume_Comparison.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile pages/2_Resume_And_Job_Description.py\nimport streamlit as st\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\nfrom fastllm import llm\n\npersist_directory = 'db'\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n\nst.set_page_config(\n    page_title=\"Job Description\",\n    page_icon=\"ðŸ“œ\"\n)\n\n# Sidebar for uploading PDF files\ndocs=[]\nwith st.sidebar:            \n    st.title(\"Resume And Job Description:\")\n    resume = st.file_uploader(\"Upload your resume \", accept_multiple_files=False)\n    jd = st.file_uploader(\"Upload your Job description \", accept_multiple_files=False)\n    if resume is None or jd is None:\n        st.stop()\n    with st.spinner(\"Processing...\"):\n\n            with open(\"resume_file.pdf\", 'wb') as f:\n                f.write(resume.getvalue())\n\n            with open(\"jd_file.pdf\", 'wb') as f:\n                f.write(jd.getvalue())\n\n            resume_path=\"/kaggle/working/resume_file.pdf\"\n            jd_path=\"/kaggle/working/jd_file.pdf\"\n\n            resume_loader = UnstructuredPDFLoader(resume_path)\n            resume_data = resume_loader.load()\n            resume_data = resume_data[0].page_content\n\n            jd_loader = UnstructuredPDFLoader(jd_path)\n            jd_data = jd_loader.load()\n            jd_data = jd_data[0].page_content \n            st.success(\"Done\")\nif len(resume_data)>1000:\n    resume_data=resume_data[0:1000]\nprompt=\"Following is the conversation between the User and AI. AI is a hiring assistant that suggests improvements in resume accorind to the job description. This time use the following job description:\"\nprompt=prompt+jd_data\nhistory=prompt\ninput_text=\"Please suggest improvements in this resume:\"+resume_data\nhistory=history+ f\"User : {input_text}\" + \"\\n\" + \"AI :\"\nresponse=llm(history, max_tokens=300)\n\nout=response[\"choices\"][0][\"text\"].strip()\nlines = history.split('\\n')\n\n\nnew_last_line = f\"AI : {out}\"\n\nlines[-1] = new_last_line\nhistory='\\n'.join(lines)\ni=history.rfind(\"AI:\")\nst.write(out)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:13:37.814260Z","iopub.execute_input":"2024-05-06T18:13:37.814679Z","iopub.status.idle":"2024-05-06T18:13:37.823749Z","shell.execute_reply.started":"2024-05-06T18:13:37.814649Z","shell.execute_reply":"2024-05-06T18:13:37.822561Z"},"trusted":true},"execution_count":243,"outputs":[{"name":"stdout","text":"Overwriting pages/2_Resume_And_Job_Description.py\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyngrok import ngrok\nngrok.set_auth_token(\"2fgafRv4DMXaDwep9sSlHpsPgeB_76vC81WKH5ns7owcmDeWC\")\n\npublic_url =  ngrok.connect(80).public_url\nprint(f\"Please click on the text below {public_url}\")\n!streamlit run --server.port 80 Homepage.py > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:14:15.489041Z","iopub.execute_input":"2024-05-06T18:14:15.490081Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Please click on the text below https://e6f9-35-223-73-195.ngrok-free.app\n/opt/conda/lib/python3.10/site-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.embeddings import SentenceTransformerEmbeddings`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/langchain/vectorstores/__init__.py:35: LangChainDeprecationWarning: Importing vector stores from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.vectorstores import Chroma`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\nllama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from zephyr-7b-beta.Q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  20:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \nllm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 2 '</s>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\nggml_cuda_init: CUDA_USE_TENSOR_CORES: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\nllm_load_tensors: ggml ctx size =    0.30 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =    70.31 MiB\nllm_load_tensors:      CUDA0 buffer size =  3847.55 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 10240\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1280.00 MiB\nllama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   692.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    28.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\nUsing fallback chat format: None\n/opt/conda/lib/python3.10/site-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.embeddings import SentenceTransformerEmbeddings`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/langchain/vectorstores/__init__.py:35: LangChainDeprecationWarning: Importing vector stores from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.vectorstores import Chroma`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n  warn_deprecated(\n/opt/conda/lib/python3.10/site-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.embeddings import SentenceTransformerEmbeddings`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/langchain/vectorstores/__init__.py:35: LangChainDeprecationWarning: Importing vector stores from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.vectorstores import Chroma`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n\nllama_print_timings:        load time =     865.33 ms\nllama_print_timings:      sample time =     103.39 ms /   184 runs   (    0.56 ms per token,  1779.76 tokens per second)\nllama_print_timings: prompt eval time =    1355.46 ms /   814 tokens (    1.67 ms per token,   600.54 tokens per second)\nllama_print_timings:        eval time =    4174.86 ms /   183 runs   (   22.81 ms per token,    43.83 tokens per second)\nllama_print_timings:       total time =    6354.40 ms /   997 tokens\n/opt/conda/lib/python3.10/site-packages/langchain/embeddings/__init__.py:29: LangChainDeprecationWarning: Importing embeddings from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.embeddings import SentenceTransformerEmbeddings`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/langchain/vectorstores/__init__.py:35: LangChainDeprecationWarning: Importing vector stores from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n\n`from langchain_community.vectorstores import Chroma`.\n\nTo install langchain-community run `pip install -U langchain-community`.\n  warnings.warn(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     865.33 ms\nllama_print_timings:      sample time =      69.36 ms /   124 runs   (    0.56 ms per token,  1787.67 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:        eval time =    2928.42 ms /   124 runs   (   23.62 ms per token,    42.34 tokens per second)\nllama_print_timings:       total time =    3472.58 ms /   125 tokens\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =     865.33 ms\nllama_print_timings:      sample time =      77.88 ms /   138 runs   (    0.56 ms per token,  1771.91 tokens per second)\nllama_print_timings: prompt eval time =     114.98 ms /    13 tokens (    8.84 ms per token,   113.07 tokens per second)\nllama_print_timings:        eval time =    3184.06 ms /   137 runs   (   23.24 ms per token,    43.03 tokens per second)\nllama_print_timings:       total time =    3903.11 ms /   150 tokens\n","output_type":"stream"}]}]}